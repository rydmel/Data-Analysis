{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOU Analysis Part 2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mwOs-GeB-hC",
        "colab_type": "text"
      },
      "source": [
        "# PREPROCESSING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVTBVNH9TyB5",
        "colab_type": "code",
        "outputId": "831ad628-89e1-47c4-e8cb-bdfc2da99152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Import libs\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LugO_35VJWc",
        "colab_type": "code",
        "outputId": "3169e1c6-9e87-4840-aaa1-adf76a373a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/64/a1df4440483df47381bbbf6a03119ef66515cf2e1a766d9369811575454b/pyspark-2.4.1.tar.gz (215.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 215.7MB 97kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 29.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/47/9b/57/7984bf19763749a13eece44c3174adb6ae4bc95b920375ff50\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc7mg4PoUxmv",
        "colab_type": "code",
        "outputId": "bba33e4b-a63c-405c-da4e-2408be6c4818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "with open('speeches.pkl', 'rb') as file:\n",
        "    data = pickle.load(file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f35d3374eac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'speeches.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'speeches.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-xVH9YGWl2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-tV71B3EFUR",
        "colab_type": "text"
      },
      "source": [
        "# 6A and 6B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p5WD-WcEO-C",
        "colab_type": "text"
      },
      "source": [
        "Using NLTK's Punkt sentence tokenizer, we are using their \"pretrained unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.\" This satisfies the nature of the part (a) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM5Z77iIZZGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Helper functions along with the transform() function which returns a list of \n",
        "tuples sorted by year. Each tuple contains the SOU year, Sentence Count, and \n",
        "Mean Sentence Length'''\n",
        "\n",
        "'''sentencing_data() returns a list of sentences for a given body of text. \n",
        "Iterating through each row in the original data file, we produce a list of \n",
        "sentences for each SOU'''\n",
        "\n",
        "def sentencing_data(str):\n",
        "  d = str.replace('\\r\\n\\r\\n', ' ') #cleans str\n",
        "  sent_list = nltk.tokenize.sent_tokenize(d)\n",
        "  return sent_list\n",
        "\n",
        "def SentLen(str):\n",
        "    spl = str.split(' ')  #finding the number of words using split()\n",
        "    return len(spl)\n",
        "  \n",
        "def avgSentLen(list):\n",
        "    sum = 0\n",
        "    for i in range(0, len(list)):\n",
        "      sum += SentLen(list[i])\n",
        "    return sum/len(list)\n",
        "\n",
        "def totalWords(list):\n",
        "  sum = 0\n",
        "  for i in range(0, len(list)):\n",
        "    sum += SentLen(list[i])\n",
        "  return sum\n",
        "\n",
        "def transform(data):\n",
        "    tup_List = []\n",
        "    for i in range(0, len(data)):\n",
        "        sentData = sentencing_data(data[i][1])\n",
        "        year = int(data[i][2])\n",
        "        numSent = len(sentData)\n",
        "        pres = data[i][0] #find name of President\n",
        "        tup_List.append((year, pres, numSent, round(avgSentLen(sentData),2), totalWords(sentData)))\n",
        "\n",
        "    return sorted(tup_List, key=lambda tup: tup[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfWwGI-IZb6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = transform(data)\n",
        "print(a)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MTE2IH2aIIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Decided to go the Pandas df route\n",
        "df = pd.DataFrame(a, columns=['Year', 'President', 'Sentences Count', 'Avg Length', 'Total Words'])\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ex90nkeA1bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array(df['Year'])\n",
        "Sent_Count= np.array(df['Sentences Count'])\n",
        "avg_length= np.array(df['Avg Length'])\n",
        "\n",
        "\n",
        "def rplot(X,Y,col):\n",
        "    M = np.vstack([X,np.ones(len(X))]).T\n",
        "    m, b = np.linalg.lstsq(M,Y)[0]\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.plot(X, Y,'+', c = '0.5')\n",
        "    F = m*X+b\n",
        "    plt.plot(X, F,'b', label = col)\n",
        "    plt.title(col)\n",
        "    \n",
        "    plt.show()\n",
        "    print('Slope: ',round(m,4) , '  Intercept: ', round(b,4)) # print slope and intercept below plot\n",
        "    \n",
        "rplot(x,Sent_Count,\"Sentences Count\")\n",
        "rplot(x,avg_length,\"Avg Length\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYy5xJB8u19P",
        "colab_type": "text"
      },
      "source": [
        "From the plots, we observe that the number of sentences in SOU addresses have increased over time (m = .7855). In addition, the average sentence length has decreased over time (m = -.1192). An intuitive explanation for this trend is that Presidents (and their speech-writers) have incorporated brevity into the speeches as the years have gone by. Instead of possibly having many run-on sentences that have more than 30 words, we see many presidents after 1950 havng a mean sentence length under 25 words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlxMAJ3znc8y",
        "colab_type": "text"
      },
      "source": [
        "# **6C**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIBVcrCRnf5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = df[:123]\n",
        "df2 = df[123:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDwL9lb_76Fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yr_pre1912 = np.array(df1['Year'])\n",
        "tw_pre1912 = np.array(df1['Total Words'])\n",
        "\n",
        "yr_post1912 = np.array(df2['Year'])\n",
        "tw_post1912 = np.array(df2['Total Words'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez__HU_Y8IU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rplot(yr_pre1912, tw_pre1912,\"Pre-1912\")\n",
        "rplot(yr_post1912, tw_post1912,\"Post-1912\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPh9vWOt_7rG",
        "colab_type": "text"
      },
      "source": [
        "Prior to 1912, we see an increase in total words (i.e. an increase of 141 words per unit increase in year via our regression).\n",
        "\n",
        "After 1912, we see no significant change in increase of total words.\n",
        "\n",
        "In 1913, Woodrow Wilson brought back in-person delivery. Due to time constraints and speech-writers' preferences, this may explain the lack of substantial increase in words.\n",
        "\n",
        "(Source: https://history.house.gov/Institution/SOTU/List/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMysBkF-EP9L",
        "colab_type": "text"
      },
      "source": [
        "# 6D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7EplFpcTdJ0",
        "colab_type": "text"
      },
      "source": [
        "To find which President has the longest sentences on average, we must consider every SOU that the President delivered. Thus, we will use a dictionary to store the *total average sentence length* , i.e. the aggregate total words divided by the aggregate number of sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kczm6dE1B_hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = {}\n",
        "\n",
        "for i in df['President'].unique():\n",
        "    totSent = 0\n",
        "    totWords = 0\n",
        "    for j in df[df['President']==i].index:\n",
        "        totWords += df['Total Words'][j]\n",
        "        totSent += df['Sentences Count'][j]\n",
        "    totavgSent = round((totWords/totSent), 2)\n",
        "    d[i] = totavgSent\n",
        "\n",
        "d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HvUKTDXUrCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_pres = max(d, key=d.get)\n",
        "min_pres = min(d, key=d.get)\n",
        "\n",
        "print(\"The President with the longest sentences on average:   \", max_pres)\n",
        "print(\"The President with the shortest sentences on average:   \", min_pres)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E07iUF5LVgsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#converts dict into list of lists, sorted by avg sent length\n",
        "\n",
        "temp = []\n",
        "dlist = []\n",
        "for key, value in d.items():\n",
        "    temp = [key,value]\n",
        "    dlist.append(temp)\n",
        "\n",
        "from operator import itemgetter\n",
        "final = sorted(dlist, key=itemgetter(1))\n",
        "final\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZquC_EhaF4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "#25th percentile\n",
        "\n",
        "print(\"The median president is:  \", final[20][0])\n",
        "q1 = math.ceil(.25 * len(final))\n",
        "q2 = math.ceil(.75 * len(final))\n",
        "print(\"The 25th percentile is:  \", final[q1-1][0])\n",
        "print(\"The 75th percentile is:  \", final[q2-1][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2UKE14gfcVH",
        "colab_type": "text"
      },
      "source": [
        "**Median**: Benjamin Harrison\n",
        "\n",
        "The 25th percentile is:   Franklin D. Roosevelt\n",
        "\n",
        "The 75th percentile is:   Zachary Taylor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3U4cgGIflNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentencing_data defined above in cell with all functions\n",
        "#intialize \n",
        "max_sent = 44  # based off longest avg sent length from Madison\n",
        "min_sent = 15\n",
        "smin = ''\n",
        "smax = ''\n",
        "for i in range(0, len(data)):\n",
        "    sentData = sentencing_data(data[i][1])\n",
        "    for j in range(0, len(sentData)):\n",
        "        if SentLen(sentData[j]) < min_sent and SentLen(sentData[j]) > 2 :   #We set this as 2 because there are many names with abbreviations which count as sentences\n",
        "            min_sent = SentLen(sentData[j])\n",
        "            smin = sentData[j]\n",
        "        if SentLen(sentData[j]) > max_sent:\n",
        "            max_sent = SentLen(sentData[j])\n",
        "            smax = sentData[j]\n",
        "\n",
        "\n",
        "print('The longest sentence:   ',smax)\n",
        "\n",
        "print('The shortest sentence:  ', smin)\n",
        "\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6DM_HWtExI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_and_split(s):\n",
        "  # encode to UTF-8, convert to lowercase and translate all hyphens and\n",
        "  # punctuation to whitespace\n",
        "  s = s.encode('utf-8').lower().replace('-',' ').translate(None, string.punctuation)\n",
        "  # replace \\r\\n\n",
        "  s = re.sub('(\\r\\n)+',' ', s)\n",
        "  # replace whitespace substrings with one whitespace and remove\n",
        "  # leading/trailing whitespaces\n",
        "  s = re.sub(' +',' ',s.strip())\n",
        "  return s.split(' ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}